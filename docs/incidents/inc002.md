inc_id: INC003
server_id: SRV002
region: North America
date: 2025-05-07T18:52:11.930Z

# What happened?
From 18:52 to 19:24 UTC the Payment Gateway staging environment experienced a full outage of callback processing. Webhook acknowledgements failed, causing partner sandbox integrations to retry aggressively. Production traffic was not impacted.

# What went wrong and why?
An OS security patch rebooted the Windows host. The staging queue processor service is configured for automatic start but failed due to a missing DLL after the patch updated a dependent runtime (Visual C++ redistributable). The service manager logged the failure but our monitoring only tracked process health via a TCP port check—which still passed because IIS started and exposed the management port, masking the worker failure. Callback messages accumulated until the backlog alarm finally triggered (lag threshold crossed after 30 minutes). By then retry storms increased queue depth and delayed recovery.

# How did we respond?
* 18:52 UTC – Host reboot occurred (automated patch cycle); service failed to initialize.
* 19:09 UTC – Queue depth alert (warning) fired but auto-closed after transient drop from partner jitter.
* 19:16 UTC – Backlog critical alert fired; incident declared SEV2.
* 19:19 UTC – Identified missing DLL in service event log; installed required redistributable version.
* 19:22 UTC – Queue processor service started; backlog draining.
* 19:24 UTC – Backlog cleared below normal threshold; incident resolved.

# How are we making incidents like this less likely or less impactful?
* Add dependency integrity check at service start with explicit metric + alert (In progress – May 2025)
* Replace port-only health check with composite liveness + worker activity probe (Planned – May 2025)
* Pre-stage runtime redistributables before patch windows (Completed)
* Introduce staggered patch scheduling to avoid simultaneous reboot of similar staging hosts (Planned – June 2025)
* Add queue backlog rate-of-change early warning alert (Completed)

# How can customers make incidents like this less impactful?
* Validate staging integration retry policies use capped exponential backoff to reduce amplification during outages.
* Monitor webhook acknowledgment latency in addition to success rate to detect partial processing stalls.
* Keep non-production environments loosely coupled so staging instability does not cascade to test suites relying on real-time callbacks.