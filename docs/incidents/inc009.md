inc_id: INC009
server_id: SRV004
region: US West
date: 2025-06-02T18:11:45Z

# What happened?
Analytics pipeline production experienced delayed batch ingestion; SLAs for hourly partitions missed between 18:11 and 19:05 UTC.

# What went wrong and why?
A maintenance window reduced the cluster size by 50% for a rolling kernel update. Rebalancing kicked off compaction jobs concurrently with ingestion, overwhelming IO and causing backpressure.

# How did we respond?
* 18:14 UTC – Batch SLA alert fired.
* 18:22 UTC – Paused compaction jobs; raised ingestion worker concurrency by 25%.
* 18:53 UTC – Backlog drained; resumed compaction with lower IO quota.
* 19:05 UTC – SLAs green; incident closed.

# How are we making incidents like this less likely or less impactful?
* Add maintenance-aware scheduler to disable compaction during scaledown (In progress)
* Enforce IO budgeting per job type (Planned)
* Pre-scale during patch windows (Completed)

# How can customers make incidents like this less impactful?
* Use eventual consistency for dashboards; avoid strict dependencies on top-of-hour data.
* Subscribe to status page for maintenance windows.
