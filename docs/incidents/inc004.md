inc_id: INC005
server_id: SRV004
region: North America
date: 2025-07-18T21:33:09.118Z

# What happened?
From 21:33 to 22:12 UTC the Production Analytics Data Pipeline experienced partial ingestion failures for real-time event shards (5 of 42 shards). Downstream dashboards showed stale metrics (up to 39 min lag) for affected dimensions.

# What went wrong and why?
During a planned maintenance window (component status: In Maintenance) we rotated IAM credentials for the object storage staging bucket. A misordered Terraform apply replaced the consumer role policy before the new access key propagated to the ingestion micro-batch jobs. Jobs attempting to rotate credentials simultaneously retried with exponential backoff but a retry bug (resetting backoff state on transient 403) created tight retry loops, exhausting per-shard concurrency. The circuit breaker thresholds were higher than actual shard traffic patterns, delaying tripping and prolonging partial failure.

# How did we respond?
* 21:33 UTC – First shard ingestion 403 errors logged; no alert (below error rate threshold).
* 21:41 UTC – Error rate crossed alert threshold; on-call paged (SEV2 declared).
* 21:48 UTC – Identified recent IAM policy change; rolled back role policy to prior version.
* 21:55 UTC – Issued manual credential propagation and restarted stuck micro-batch jobs.
* 22:06 UTC – Shard backlog draining; lag decreasing steadily.
* 22:12 UTC – Lag < 5 min across all shards; incident resolved.

# How are we making incidents like this less likely or less impactful?
* Add pre-change validation ensuring new credentials active before policy revocation (Planned – Aug 2025)
* Fix retry backoff state reset bug (Completed)
* Lower circuit breaker thresholds for shard ingestion failure ratio (Planned – Aug 2025)
* Introduce canary shard to validate credential rotation before global apply (In progress – Aug 2025)
* Add dashboard and alert for credential propagation latency (Planned – Aug 2025)

# How can customers make incidents like this less impactful?
* Design analytics queries to degrade gracefully (e.g., show partial data with freshness annotation) when some shards are delayed.
* Leverage export APIs to reconcile events once backfill completes, avoiding manual data patching.
* Use alerting on freshness / lag metrics rather than only absolute counts to catch partial ingestion issues.