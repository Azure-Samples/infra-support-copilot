inc_id: INC007
server_id: SRV002
region: US East
date: 2025-05-12T14:03:12Z

# What happened?
Payment gateway staging experienced elevated checkout failures (~12% 4xx/5xx mix) and increased latency during load testing between 14:03 and 14:31 UTC.

# What went wrong and why?
A firewall policy update blocked outbound connections to a third-party BIN lookup service used for fraud checks, causing timeouts and fallback paths that incorrectly treated timeouts as hard failures.

# How did we respond?
* 14:06 UTC – QA reported spike in checkout failures; on-call engaged.
* 14:12 UTC – Traced failures to outbound egress block; identified rule change in last policy push.
* 14:18 UTC – Implemented temporary allow rule for BIN service; retried failed test cases.
* 14:27 UTC – Latency and failure rates returned to baseline.
* 14:31 UTC – Incident resolved; action items captured.

# How are we making incidents like this less likely or less impactful?
* Add egress allowlist CI check comparing IaC and live dependencies (Planned – Sep 2025)
* Treat egress timeouts as soft-fail with degraded path instead of hard-fail (In progress)
* Add synthetic canary to validate external dependencies prior to tests (Completed)

# How can customers make incidents like this less impactful?
* Use idempotent payment initialization with client-side retries.
* Implement clear user messaging when payment verification is delayed or requires retry.
