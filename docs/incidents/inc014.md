inc_id: INC014
server_id: SRV009
region: Asia Southeast
date: 2025-07-01T22:09:55Z

# What happened?
ML batch workers in dev failed to start two scheduled runs; jobs stuck in Pending for ~23 minutes.

# What went wrong and why?
GPU node taints were applied in the dev cluster without corresponding tolerations in the job spec. The scheduler couldn’t place pods on GPU nodes.

# How did we respond?
* 22:12 UTC – Job SLA alert fired; developer on-call engaged.
* 22:17 UTC – Added tolerations to job spec and requeued.
* 22:32 UTC – Jobs running; incident closed.

# How are we making incidents like this less likely or less impactful?
* Add admission controller to enforce required tolerations for GPU jobs (In progress)
* Provide fallback CPU implementation for small batches (Planned)
* Preflight validation step in CI (Completed)

# How can customers make incidents like this less impactful?
* Schedule non-urgent dev runs outside peak hours.
* Implement smaller shards to reduce impact of placement delays.
