inc_id: INC012
server_id: SRV007
region: US Central
date: 2025-02-21T16:58:07Z

# What happened?
UAT web frontend showed intermittent 502 errors during a blue/green deployment window between 16:58 and 17:12 UTC.

# What went wrong and why?
Health probe path changed in the new release but the load balancer configuration wasn’t updated. Half of the new instances failed health checks and were cycled repeatedly, causing request drops.

# How did we respond?
* 17:00 UTC – Automated health probe failure alert fired.
* 17:05 UTC – Corrected probe path in load balancer config and warmed instances.
* 17:10 UTC – Error rate back to baseline; incident closed at 17:12 UTC.

# How are we making incidents like this less likely or less impactful?
* Enforce config drift checks between app and infra manifests (Completed)
* Add deployment guard to validate health probe before cutover (In progress)
* Extend canary window from 2 to 5 minutes (Completed)

# How can customers make incidents like this less impactful?
* Retry navigation actions when receiving a 502 during deploy windows.
* Subscribe to deployment windows for UAT environment.
